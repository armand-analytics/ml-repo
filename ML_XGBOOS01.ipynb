{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install xgboost==1.7.1\n",
    "# import xgboost as xgb\n",
    "# print(\"Versión de XGBoost:\", xgb.__version__)\n",
    "# PREPROCESS\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve \n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "df=pd.read_csv(r'C:\\Users\\ARMANDO\\Documents\\data\\dataset_full.csv')\n",
    "#StandardScaler Standardize features by removing the mean and scaling to unit variance.The standard score of a sample x is calculated as:\n",
    "#z = (x - u) / s\n",
    "#varianza=1 mu=0\n",
    "scaler = StandardScaler() # mi escalador de features numericos\n",
    "boolean_features = ['email_in_url', 'domain_in_ip', 'server_client_domain', 'tld_present_params', 'domain_spf', 'tls_ssl_certificate', 'url_google_index', 'domain_google_index', 'phishing']\n",
    "#el d1 es el dataset sin los booleans\n",
    "d1=df.drop(boolean_features, axis=1)\n",
    "#Como identificamos facilmente los booleans que son pocos,  solo falta quedarnos con el \"antijoin\" de los booleans\n",
    "#sacamos columnas que no son booleanas y que son numéricas | #\"Return a new Index with elements of index not in other.This is the set difference of two Index objects.\" osea  no en booleans\n",
    "numcols=d1.columns.difference(boolean_features)\n",
    "#escalamos todos los que no son booleanos \"numcols ya es una lista\" aplicamos fittransform\n",
    "#fit_transform() generalmente se usa en transformadores como StandardScaler de scikit-learn \n",
    "#para ajustar la escala de las características y luego transformar los datos en consecuencia.\n",
    "#fit: Se ajusta el transformador a los datos de entrenamiento. En el contexto de StandardScaler, \n",
    "# esto calcula la media y la desviación estándar de cada característica en los datos de entrenamiento.\n",
    "#transfromr aplica la transformación a los datos de entrenamiento\n",
    "# como son un metodo de mi scaler la fit-transformación en cuestión es un método de mi scaler (standarizador)\n",
    "\n",
    "d1[numcols]=scaler.fit_transform(d1[numcols])\n",
    "#al concatenar tenemos nuestros datos homogeneos\n",
    "TrainDs=pd.concat([d1, df[boolean_features]], axis=1)\n",
    "target=TrainDs['phishing']\n",
    "#features=features.drop('phishing',axis=1 )\n",
    "kBest = SelectKBest(score_func=f_classif, k=75)\n",
    "Best_30 = kBest.fit_transform(TrainDs, target)\n",
    "Best_30 = TrainDs.columns[kBest.get_support()].tolist()\n",
    "scores=kBest.scores_\n",
    "kBest = pd.DataFrame({'Feature': Best_30, 'Score': scores[kBest.get_support()]})\n",
    "Best30 = kBest.sort_values(by= 'Score', ascending = False)[:30].reset_index(drop=True)\n",
    "#Best30=Best30[1:].index\n",
    "#corr_cols = Best30.index\n",
    "corr_cols = Best30['Feature'].tolist()\n",
    "x_train = TrainDs[corr_cols].drop('phishing', axis = 1)\n",
    "y_train = TrainDs['phishing']\n",
    "print('fin de preporcessor,  comienza xgboosting')\n",
    "#PROCESSOR\n",
    "#TRAIN\n",
    "#test_size=0.2 significa que el 20% de los datos serán para el conjunto prueba mientras que el 80% para training\n",
    "# Se utiliza comúnmente el número 42, pero podrías usar \n",
    "# cualquier número entero que desees. La idea es que al usar el mismo valor de random_state, \n",
    "# obtendrás siempre la misma división de datos\n",
    "params = {'objective': 'binary:logistic','eval_metric': 'logloss'}\n",
    "a, X_test, y_train, y_test = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "num_round = 100  # Número de iteraciones de entrenamiento\n",
    "\n",
    "model = xgb.train(params, dtrain, num_round)\n",
    "# Realizar predicciones en el conjunto de prueba\n",
    "y_pred = model.predict(dtest)\n",
    "# de probabilidades a binarios (0 o 1)\n",
    "y_pred_binary = [1 if prob >= 0.5 else 0 for prob in y_pred]\n",
    "# Calculando la precisión del modelo\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "print('exito!')\n",
    "print(\"Accuracy:\", accuracy)\n",
    "y_pred_binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación hace un dump del modelo y se empaqueta en un modelo tar_gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import joblib\n",
    "\n",
    "ruta_modelo = 'C:/Users/ARMANDO/Documents/Python Scripts/model.joblib'\n",
    "# Guardar el modelo en un archivo Joblib\n",
    "joblib.dump(model, ruta_modelo)\n",
    "\n",
    "def compress_to_tar_gz(file_to_compress, output_tar_gz):\n",
    "    with tarfile.open(output_tar_gz, \"w:gz\") as tar:\n",
    "        tar.add(file_to_compress, arcname=file_to_compress.split(\"/\")[-1])\n",
    "\n",
    "# Ruta del archivo que quieres comprimir\n",
    "file_to_compress = 'C:/Users/ARMANDO/Documents/Python Scripts/model.joblib'\n",
    "\n",
    "# Ruta y nombre del archivo comprimido .tar.gz\n",
    "output_tar_gz = 'C:/Users/ARMANDO/Documents/Python Scripts/model_29f.tar.gz'\n",
    "\n",
    "# Comprimir el archivo\n",
    "compress_to_tar_gz(file_to_compress, output_tar_gz)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
